---
Title: k8s クラスタを kubeadm でアップグレードしたときのメモ
Category:
- kubernetes
Date: 2019-07-20T18:45:26+09:00
URL: https://nabeop.hatenablog.com/entry/2019/07/20/184526
EditURL: https://blog.hatena.ne.jp/nabeop/nabeop.hatenablog.com/atom/entry/26006613375274578
---

最近は手元で k8s クラスタを作って遊んでいます。いつの間にか k8s の 1.15 系が使えるようになっていたので、kubeadm を使ってクラスタのアップグレードをしてみました。

手順は以下の kubeadm を使った 1.14 から 1.15 へのアップグレード手順をなぞっただけです。

[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/:embed]

実際に実施した時の手順のメモは以下。

[https://github.com/nabeo/vagrant-k8s-metallb/blob/6fb0136af09b448a1e94bd48999259dcb3b36d84/README.md#kubeadm-%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6-k8s-%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%81%AE%E3%82%A2%E3%83%83%E3%83%97%E3%83%87%E3%83%BC%E3%83%88:embed]

特に複雑なことはしていないクラスタだったので、すんなりアップグレードできてしまった。

ただ、アップグレードが終わった後に `kubectl get all` とかしていると特定の worker node に割り当たっている coredns の Pod が起動失敗している気配があったので、kubeadm を使って該当 worker node をクラスタから離脱させて再参加させるようなことをしていました。
